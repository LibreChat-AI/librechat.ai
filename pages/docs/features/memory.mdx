---
title: Memory
description: Enable conversation memory and personalization features in LibreChat
---

# Memory

## Overview

Memory in LibreChat allows the system to remember information across conversations, providing a more personalized and context-aware experience. When enabled, the AI can recall user preferences, important facts, and conversation context to enhance future interactions.

<Callout type="important" title="⚠️ Configuration Required">
  - Memory functionality must be explicitly configured in your `librechat.yaml` file to work. It is
  not enabled by default.
</Callout>

## Key Features

- **Persistent Context**: Information learned in one conversation can be recalled in future conversations
- **User Control**: When enabled, users can toggle memory on/off for their individual chats
- **Customizable Storage**: Control what types of information can be stored using valid keys
- **Token Management**: Set limits on memory usage to control costs
- **Agent Integration**: Use AI agents to intelligently manage what gets remembered

## Configuration

To enable memory features, you need to add the `memory` configuration to your `librechat.yaml` file:

```yaml filename="librechat.yaml"
version: 1.2.8
cache: true

memory:
  disabled: false # Set to true to completely disable memory
  personalize: true # Gives users the ability to toggle memory on/off, true by default
  tokenLimit: 2000 # Maximum tokens for memory storage
  messageWindowSize: 5 # Number of recent messages to consider
  agent:
    provider: 'openAI'
    model: 'gpt-4'
```

The provider field should match the accepted values as defined in the [Model Spec Guide](/docs/configuration/librechat_yaml/object_structure/model_specs#endpoint).

**Note:** If you are using a custom endpoint, the endpoint value must match the defined custom endpoint name exactly.

See the [Memory Configuration Guide](/docs/configuration/librechat_yaml/object_structure/memory) for detailed configuration options.

## How It Works

<Callout type="note" title="Memory Agent Execution">
Until smart detection of memory-relevant requests (beyond the LLM's own agency) is implemented, the memory agent runs on every request when memory is enabled.

Timing and order:

- The memory agent runs before the main chat response.
- It is limited to the amount of time it takes the main chat request to completely finish, plus up to 3 seconds after the initial chat request finishes.
- This design ensures memory updates are applied promptly and are expected to finish sooner if updates are needed, without delaying the user's chat experience.
  </Callout>

### 1. Information Storage

When memory is enabled, the system can store:

- User preferences (communication style, topics of interest)
- Important facts and context from conversations
- Personal information explicitly shared by users
- Ongoing projects or tasks mentioned

### 2. Context Window

The `messageWindowSize` parameter determines how many recent messages are analyzed for memory updates. This helps the system decide what information is worth remembering.

### 3. User Control

When `personalize` is set to `true`:

- Users see a memory toggle in their chat interface
- They can enable/disable memory for individual conversations
- Memory settings persist across sessions

### 4. Valid Keys

You can restrict what types of information are stored by specifying `validKeys`:

```yaml filename="memory / validKeys"
memory:
  validKeys:
    - 'user_preferences'
    - 'conversation_context'
    - 'learned_facts'
    - 'personal_information'
```

## Best Practices

### 1. Token Limits

Set appropriate token limits to balance functionality with cost:

- Higher limits allow more comprehensive memory
- Lower limits reduce processing costs
- Consider your usage patterns and budget

### 2. Custom Instructions

When using `validKeys`, provide custom instructions to the memory agent:

```yaml filename="memory / agent with instructions"
memory:
  agent:
    provider: 'openAI'
    model: 'gpt-4'
    instructions: |
      Store information only in the specified validKeys categories.
      Focus on explicitly stated preferences and important facts.
      Delete outdated or corrected information promptly.
```

### 3. Privacy Considerations

- Memory stores user information across conversations
- Ensure users understand what information is being stored
- Consider implementing data retention policies
- Provide clear documentation about memory usage

## Examples

### Basic Configuration

Enable memory with default settings:

```yaml filename="Basic memory config"
memory:
  tokenLimit: 2000
  agent:
    provider: 'openAI'
    model: 'gpt-4.1-mini'
```

### Advanced Configuration

Full configuration with all options:

```yaml filename="Advanced memory config"
memory:
  disabled: false
  validKeys: ['preferences', 'context', 'facts']
  tokenLimit: 3000
  personalize: true
  messageWindowSize: 10
  agent:
    provider: 'anthropic'
    model: 'claude-3-opus-20240229'
    instructions: 'Remember only explicitly stated preferences and key facts.'
    model_parameters:
      temperature: 0.3
```

For valid model parameters per provider, see the [Model Spec Preset Fields](/docs/configuration/librechat_yaml/object_structure/model_specs#preset-fields).

### Using Predefined Agents

Reference an existing agent by ID:

```yaml filename="Memory with agent ID"
memory:
  agent:
    id: 'memory-specialist-001'
```

## Troubleshooting

### Memory Not Working

1. Verify memory is configured in `librechat.yaml`
2. Check that `disabled` is set to `false`
3. Ensure the configured agent/model is available
4. Verify users have enabled memory in their chat interface

### High Token Usage

1. Reduce `tokenLimit` to control costs
2. Decrease `messageWindowSize` to analyze fewer messages
3. Use `validKeys` to restrict what gets stored
4. Review and optimize agent instructions

### Inconsistent Memory

1. Check if users are toggling memory on/off
2. Verify token limits aren't being exceeded
3. Ensure consistent agent configuration
4. Review stored memory for conflicts

## Related Features

- [Agents](/docs/features/agents) - Build custom AI assistants
- [Presets](/docs/user_guides/presets) - Save conversation settings
- [Fork Messages](/docs/features/fork) - Branch conversations while maintaining context
