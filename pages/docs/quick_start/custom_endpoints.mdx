import AdditionalLinks from '@/components/repeated/AdditionalLinks.mdx';

# Custom Endpoints

LibreChat supports OpenAI API compatible services using the `librechat.yaml` configuration file.

This guide assumes you have already set up LibreChat using Docker, as shown in the **[Local Setup Guide](/docs/quick_start/local_setup).**

<Callout type="info" title="Configuration Files Clarification">
LibreChat uses several configuration files, each with specific purposes:

1. **librechat.yaml** - Used for custom endpoints configuration and other application settings
2. **.env file** - Used for server configuration, pre-configured endpoint API keys, and authentication settings
3. **docker-compose.override.yml** - Used for Docker-specific configurations and mounting volumes
</Callout>

## Step 1. Create or Edit a Docker Override File

- Create a file named `docker-compose.override.yml` file at the project root (if it doesn't already exist).
- Add the following content to the file:

```yaml
services:
  api:
    volumes:
    - type: bind
      source: ./librechat.yaml
      target: /app/librechat.yaml
```

> Learn more about the [Docker Compose Override File here](/docs/configuration/docker_override).

## Step 2. Configure `librechat.yaml`

- **Create a file named `librechat.yaml`** at the project root (if it doesn't already exist).
- **Add your custom endpoints:** you can view compatible endpoints in the [AI Endpoints section](/docs/configuration/librechat_yaml/ai_endpoints).
  - The list is not exhaustive and generally every OpenAI API-compatible service should work.
  - There are many options for Custom Endpoints. View them all here: [Custom Endpoint Object Structure](/docs/configuration/librechat_yaml/object_structure/custom_endpoint).
- As an example, here is a configuration for both **OpenRouter** and **Ollama**:

   ```yaml
   version: 1.2.8
   cache: true
   endpoints:
     custom:
       - name: "OpenRouter"
         apiKey: "${OPENROUTER_KEY}"
         baseURL: "https://openrouter.ai/api/v1"
         models:
           default: ["gpt-3.5-turbo"]
           fetch: true
         titleConvo: true
         titleModel: "current_model"
         summarize: false
         summaryModel: "current_model"
         forcePrompt: false
         modelDisplayLabel: "OpenRouter"
       - name: "Ollama"
         apiKey: "ollama"
         baseURL: "http://host.docker.internal:11434/v1/"
         models:
           default: [
             "llama3:latest",
             "command-r",
             "mixtral",
             "phi3"
             ]
           fetch: true # fetching list of models is not supported
         titleConvo: true
         titleModel: "current_model"
   ```

<Callout type="warning" title="Important: API Key Configuration">
When configuring API keys in custom endpoints, you have two options:

1. **Environment Variable Reference**: Use `${VARIABLE_NAME}` syntax to reference a variable from your .env file (recommended for security)
   ```yaml
   apiKey: "${OPENROUTER_KEY}"
   ```

2. **User Provided**: Set to `"user_provided"` (without the ${} syntax) to allow users to enter their own API key through the web interface
   ```yaml
   apiKey: "user_provided"
   ```

3. **Direct Value**: Directly include the API key in the configuration file (not recommended for security reasons)
   ```yaml
   apiKey: "your-actual-api-key"
   ```

This is different from pre-configured endpoints in the .env file where you would set `ENDPOINT_KEY=user_provided` (e.g., `OPENAI_API_KEY=user_provided`).
</Callout>

## Step 3. Configure .env File

- **Edit your existing `.env` file** at the project root 
    - Copy `.env.example` and rename to `.env` if it doesn't already exist.
- According to the config above, the environment variable `OPENROUTER_KEY` is expected and should be set:

```bash
OPENROUTER_KEY=your_openrouter_api_key
```

**Notes:**
- As way of example, this guide assumes you have setup Ollama independently and is accessible to you at `http://host.docker.internal:11434`
    - "host.docker.internal" is a special DNS name that resolves to the internal IP address used by the host.
    - You may need to change this to the actual IP address of your Ollama instance.
- In a future guide, we will go into setting up Ollama along with LibreChat.

## Step 4. Run the App

- Now that your files are configured, you can run the app:

```bash
docker compose up
```

Or, if you were running the app before, you can restart the app with:

```bash
docker compose restart
```

> Note: Make sure your Docker Desktop or Docker Engine is running before executing the command.

## Conclusion

**That's it!** You have now configured **Custom Endpoints** for your LibreChat instance. 

<AdditionalLinks />
