---
title: vLLM
description: Example configuration for vLLM
---

# [vLLM](https://github.com/vllm-project/vllm)

> vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs.

**Notes:**

- **Not Known:** icon not provided, but fetching list of models is recommended to get available models from your local vLLM server.

- The `titleMessageRole` is important as some local LLMs will not accept system message roles for title messages (which is the default).

- This configuration assumes you have a vLLM server running locally at the specified baseURL.

```yaml
    - name: "vLLM"
      apiKey: "vllm"
      baseURL: "http://127.0.0.1:8023/v1"
      models:
        default: ['google/gemma-3-27b-it']
        fetch: true
      titleConvo: true
      titleModel: "current_model"
      titleMessageRole: "user"
      summarize: false
      summaryModel: "current_model"
      forcePrompt: false
```

The configuration above connects LibreChat to a local vLLM server running on port 8023. It uses the Gemma 3 27B model as the default model, but will fetch all available models from your vLLM server.

## Key Configuration Options

- `apiKey`: A simple placeholder value for vLLM (local deployments typically don't require authentication)
- `baseURL`: The URL where your vLLM server is running
- `titleMessageRole`: Set to "user" instead of the default "system" as some local LLMs don't support system messages
