title: Bifrost
description: Example configuration for Bifrost

# [Bifrost Docs](https://docs.getbifrost.ai/)

**Notes:**
- Reference [Using LibreChat with Bifrost](https://docs.getbifrost.ai/quickstart/gateway/cli-agents#librechat) for configuration.

## Add Bifrost as a custom provider
Once you have LibreChat installed, you can add Bifrost as a custom provider.
Add the following to your librechat.yaml file:

```yaml
version: 1.2.8
cache: true

endpoints:
  custom:
    - name: "Bifrost"
      apiKey: "dummy" # Add the authentication key if login is enabled, otherwise add a placeholder
      baseURL: "http://host.docker.internal:8080/v1" # Or localhost:8080 if running locally, or {your-bifrost-container}:8080 if running in the same docker network
      models:
        default: ["openai/gpt-4o"] # Replace with the model you want to use
        fetch: true
      titleConvo: true
      titleModel: "openai/gpt-4o" # Replace with the model you want to use for chat title generation
      summarize: false # Set to true if you want to enable chat summary generation
      summaryModel: "openai/gpt-4o" # Replace with the model you want to use for chat summary generation
      forcePrompt: false # Set to true if you want to enable force prompt generation
      modelDisplayLabel: "Bifrost" 
      iconURL: https://getbifrost.ai/bifrost-logo-only.png
   ```

Run LibreChat
Now you can start using Bifrost as a provider in LibreChat, with all the features of Bifrost.

![image](https://getbifrost.ai/librechat-ss.png)
